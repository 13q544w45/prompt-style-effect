
# The Impact of Prompting Style on LLM In-Context-Learning Performance

## 1. Project Overview

This project evaluates the performance differences of various prompt methodsâ€”**Basic Prompt**, **Instruction Prompt**, and **Chain-of-Thought (CoT) Prompt**â€”across multiple natural language processing (NLP) tasks, including:

- Winograd-Style Task
- Closed-Book Question Answering
- Translation
- Natural Language Inference (NLI)

It aims to support the design of better prompts to enhance large language modelsâ€™ (LLMs) performance in **in-context learning** through data analysis and visualization.

---

## 2. Background and Research Objectives

One effective way to improve LLM output is to include examples in the prompt, a method known as **few-shot prompting**. Common prompt styles include:

### ğŸŸ¢ Basic QA Prompt
```
Q: What is the capital of France?  
A: Paris  
Q: Who wrote the play 'Hamlet'?  
A: William Shakespeare  
Q: What is the boiling point of water in Celsius?  
A: 100  
Q: What was the first name of Hudson in Upstairs Downstairs?  
A:  
```

### ğŸ”µ Instruction Prompt
```
Instruction: Answer the question based only on your general knowledge.  
Q: What is the capital of France?  
A: Paris  

Instruction: Answer the question using only facts you know.  
Q: Who wrote the play 'Hamlet'?  
A: William Shakespeare  
...
```

### ğŸŸ¡ Chain-of-Thought Prompt
```
Instruction: Answer the question based only on your general knowledge.  
Q: What is the capital of France?  
A: France is a country in Europe. Its capital city is where the national government is located... The answer is Paris.  
...
```

Prompt engineering is one of the most **cost-effective** and **efficient** ways to enhance LLM performance. However, systematic, quantitative comparisons of prompt styles remain limited.

This project explores:

1. To what extent does prompt style affect GPT-3.5-Turbo performance?
2. Is there a consistently optimal prompt style?

---

## 3. How to Run the Experiment

Based on findings from *Large Language Models as Analogical Reasoners*, using **3 to 5 examples** (K=3â€“5) yields the best performance across tasks.

- Three examples are manually crafted for each prompt style.
- GPT-3.5 is tested with identical questions using different prompt styles.
- Performance is measured by accuracy or BLEU scores.

---

## 4. Project Structure

```
Prompt-Strategy-Eval/
â”œâ”€â”€ data/               # Task datasets
â”œâ”€â”€ generated_prompts/  # Initial prompts and answers
â”œâ”€â”€ model_inputs/       # Prompts + questions
â”œâ”€â”€ model_outputs/      # LLM-generated responses
â”œâ”€â”€ sample-dataset/     # 100 sample questions
â”œâ”€â”€ evaluation/         # Evaluation logic
â”œâ”€â”€ prompts/            # Three styles of prompts
â”œâ”€â”€ scripts/            # Data processing & evaluation
â”œâ”€â”€ clean/              # Preprocessing scripts
â”œâ”€â”€ closedbookqa/       # Prompts for QA tasks
â”œâ”€â”€ evaluate/           # Evaluation scripts
â”œâ”€â”€ nli/                # NLI task prompts
â”œâ”€â”€ run_gpt3/           # GPT-3.5 invocation scripts
â”œâ”€â”€ samples/            # Sample extraction
â”œâ”€â”€ translation/        # Translation task prompts
â”œâ”€â”€ winogrande/         # Winograd task prompts
```

---

## 5. Results and Findings

### Closed-Book QA Task
- **Instruction Prompt** performed best (46% accuracy).
- **Basic** and **CoT Prompts** slightly lower (43%).

![Closed Book QA](./images/closedbookqa_results.png)

### NLI Task
- **Basic Prompt**: 80% accuracy  
- **Instruction Prompt**: 72%  
- **CoT Prompt**: 69%

![NLI Results](./images/nli_results.png)

### Translation Task
Measured by **BLEU Score**:

| Prompt Style     | BLEU Score |
|------------------|------------|
| Basic Prompt     | 0.2852     |
| Instruction      | 0.2809     |
| Chain-of-Thought | 0.2767     |

![Translation Results](./images/translation_results.png)

### Winograd Task
- **Basic Prompt**: 79% accuracy  
- **Instruction / CoT**: 75% accuracy

![Winograd Results](./images/winogrande_results.png)

---

## ğŸ” Conclusion

Prompt style **does influence** LLM performance, but its effect **varies by task**. From the tasks examined:

- **Basic Prompts** often yielded the best results.
- No single prompt style guarantees optimal performance across all tasks.
- It's best to **experiment with multiple styles** based on task complexity and expected output.

---

## ğŸ“ License

MIT License

---
